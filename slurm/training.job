#!/bin/sh
#SBATCH --partition=GPUQ                  # GPU partition
#SBATCH --account=ie-idi                  # Replace with your valid account
#SBATCH --time=72:00:00                   # Set the time limit for the sweep
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --gres=gpu:1                      # Request 1 GPU
#SBATCH --mem=64G                         # Request 64GB memory
#SBATCH --job-name="Bert4Nilm_Training"
#SBATCH --output=training.log                # Output log file
#SBATCH --mail-user=farlond@stud.ntnu.no  # Email for notifications
#SBATCH --mail-type=ALL

# Set the working directory
cd ${SLURM_SUBMIT_DIR}

# Print job information
echo "Running from the directory: $SLURM_SUBMIT_DIR"
echo "                  job name: $SLURM_JOB_NAME"
echo "                    Job ID: $SLURM_JOB_ID"
echo "                Used nodes: $SLURM_JOB_NODELIST"
echo "           Number of nodes: $SLURM_JOB_NUM_NODES"
echo "       Number of CPU cores: $SLURM_CPUS_ON_NODE"
echo "        CPU cores per node: $SLURM_CPUS_ON_NODE"
echo "          Total used cores: $SLURM_NTASKS"

# Load Anaconda and activate the Conda environment
module load Anaconda3/2023.09-0
module load cuDNN/8.5.0.96-CUDA-11.7.0
conda activate nnenv

# Enables WandB and ensurs it syncs with the cloud:
wandb enabled
wandb online

# Triggers the training script:
python ../src/train.py

# Display system information for debugging
uname -a

